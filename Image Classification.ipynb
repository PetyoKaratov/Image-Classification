{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from skimage.io import imread\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "link to data we use:\n",
    "https://drive.google.com/open?id=1xsi7LtfirtGIh3FqiGUmXoOzQWnZF6WZ\n",
    "You should add the downloaded folder to the project main folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes from [Kaggle](https://www.kaggle.com/chetankv/dogs-cats-images). It was originally split into `training` and `testing` sets but they were merged togerther for the purpose of demonstrating how to create our own data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = \"data\"\n",
    "SPLITS_DIR = \"splits\"\n",
    "\n",
    "CAT_CLASS, DOG_CLASS = \"cat\", \"dog\"\n",
    "\n",
    "TRAIN_PCT, VAL_PCT, TEST_PCT = 0.9, 0.05, 0.05\n",
    "\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting and cleaning data\n",
    "First, we need to get all filenames. It's impractical to load all images in memory since the dataset can be really large. We can keep track of the filenames and \"materialize them\" (i.e. read the images) only when we need to do so, one mini-batch at a time. This is one of many examples of *lazy execution* we've seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_filenames(base_dir, class_names_plural = True):\n",
    "    \"\"\"\n",
    "    Returns the filenames and their corresponding classes.\n",
    "    Assumes the following structure:\n",
    "    |---base_dir\n",
    "    | |---class1\n",
    "    | | |---image1.jpg\n",
    "    | | |---image2.jpg\n",
    "    | |---class2\n",
    "    | | |---image1.jpg\n",
    "    | | |---image2.jpg\n",
    "    \n",
    "    Since in our case we can infer the class from the filename,\n",
    "    we could skip returning it but we'll do so for clarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    filenames = {\n",
    "        \"image_filename\": [],\n",
    "        \"image_class\": []\n",
    "    }\n",
    "    \n",
    "    for image_class in os.listdir(base_dir):\n",
    "        image_class_dir = os.path.join(base_dir, image_class)\n",
    "        filenames_in_class = [os.path.join(image_class_dir, file) for file in os.listdir(image_class_dir)]\n",
    "        filenames[\"image_filename\"].extend(filenames_in_class)\n",
    "        \n",
    "        normalized_image_class = image_class[:-1] if class_names_plural else image_class\n",
    "        filenames[\"image_class\"].extend([normalized_image_class] * len(filenames_in_class))\n",
    "    return pd.DataFrame(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = get_all_filenames(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the names are more than 10 000 which is unexpected. A simple regex on the filenames shows that there are duplicate filenames, e.g. `data\\cats\\cat.4142(1).jpg`. We can see that there are exactly 28 of them, and they contain the exact same information.\n",
    "\n",
    "We can assume that no more \"contaminations\" of the dataset exist although it might be useful to check for duplicate images with different filenames, or similar images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = filenames[~filenames.image_filename.str.contains(\"\\(\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration\n",
    "This is not meant to be a comprehensive exploration, just some really simple checks. As usual, we need to see the distribution of classes, image resolution (number of pixels), colors (if there's a mix of grayscale and color images, it can throw off our algorithm and we need to take it into account), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.groupby(\"image_class\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_dimensions(image_filename):\n",
    "    \"\"\"\n",
    "    Returns the dimensions of the image (height, width, channels) in pixels.\n",
    "    \n",
    "    There are better methods which don't involve reading the entire image\n",
    "    and loading it in memory but this is simple enough.\n",
    "    \"\"\"\n",
    "    return imread(image_filename).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = filenames.image_filename.apply(get_image_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = pd.DataFrame([*dimensions], columns = [\"height\", \"width\", \"channels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dimensions[dimensions.channels != 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (h_ax, w_ax, res_ax) = plt.subplots(1, 3, figsize = (18, 6))\n",
    "h_ax.hist(dimensions.height, bins = 30)\n",
    "h_ax.set_xlabel(\"Image height\")\n",
    "\n",
    "w_ax.hist(dimensions.width, bins = 30)\n",
    "w_ax.set_xlabel(\"Image width\")\n",
    "\n",
    "res_ax.hist(dimensions.width / dimensions.height, bins = 40)\n",
    "res_ax.set_xlabel(\"Aspect ratio\")\n",
    "\n",
    "plt.suptitle(\"Distributions of image dimensions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the images are pretty small and they are close to a usual `4:3` ratio.\n",
    "\n",
    "An in-depth data analysis would try to find specific \"harder\" examples, e.g. animals behind a cage, in strange positions, too small (or too large) images, images where the animal is not clearly visible, etc. It would also try to find mislabeled data (and if possible, quantify the percentage of wrong labels)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing sets\n",
    "We could split the data in multiple ways. One recommended way is to use a pre-defined split. That is, shuffle the data (use stratified shuffling when necessary), split, and write the results into a file. After that, it gets much easier to load the files, and the splits won't incur additional randomness in the model performance.\n",
    "\n",
    "A relatively good split seems to be 90% / 5% / 5%. This leaves 250 images from each class for validation and testing while maximizing the amount of training images.\n",
    "\n",
    "**Note:** We could make the function much more abstract by allowing it to create an arbitrary number of splits. Then, we simply need to iterate over them. This will allow repeated code (dataframe initialization, concatenation, final shuffling) to be replaced by a for-loop. However, it will make the code harder to understand from a scientific point of view. We always need to balance both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "for class_name, data in filenames.groupby(\"image_class\"):\n",
    "    # Randomize the order (shuffle) before splitting\n",
    "    data = data.sample(frac = 1)\n",
    "    train_end_index, val_end_index = round(TRAIN_PCT * len(data)), round((TRAIN_PCT + VAL_PCT) * len(data))\n",
    "    train_data_in_class, val_data_in_class, test_data_in_class = \\\n",
    "        data[:train_end_index], data[train_end_index: val_end_index], data[val_end_index:]\n",
    "    train_data = pd.concat([train_data, train_data_in_class])\n",
    "    val_data = pd.concat([val_data, val_data_in_class])\n",
    "    test_data = pd.concat([test_data, test_data_in_class])\n",
    "    \n",
    "# Randomize the sets once again so the classes are not consecutive\n",
    "train_data = train_data.sample(frac = 1)\n",
    "val_data = val_data.sample(frac = 1)\n",
    "test_data = val_data.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can save the datasets and load them from files later (so we don't have to regenerate them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(SPLITS_DIR):\n",
    "    os.makedirs(SPLITS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, filename in zip([train_data, val_data, test_data], [\"train\", \"val\", \"test\"]):\n",
    "    dataset.to_csv(os.path.join(SPLITS_DIR, filename + \".csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing images for modelling\n",
    "Because of the specific of the model we're going to use (`Dense` layers), we'll need to make sure all images are the same size before passing them. The easiest way to do this is to resize them to a fixed size. This will stretch or squish them but the model will still be able to learn from them.\n",
    "\n",
    "In practice, we usually do the image resizing beforehand so we don't have to do it every time we're reading a particular image. This is to say, we're *caching* the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensorflow` works with many dataset formats. The one we'd like to use is a tuple `(image_data, image_class)` where `image_data` is a 3D tensor, and `image_class` is a number (0 for `dog` / 1 for `cat` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(SPLITS_DIR, \"train.csv\"))\n",
    "val_data = pd.read_csv(os.path.join(SPLITS_DIR, \"val.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(SPLITS_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_prepare_image(image_filename, image_class):\n",
    "    # Get image\n",
    "    image = tf.io.read_file(image_filename)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    \n",
    "    # Resize and normalize\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    \n",
    "    def preprocess_image(x):\n",
    "        \"\"\"\n",
    "        This is a stripped-down version of Keras' own imagenet preprocessing function,\n",
    "        as the original one is throwing an exception\n",
    "        \"\"\"\n",
    "\n",
    "        backend = tf.keras.backend\n",
    "    \n",
    "        # 'RGB'->'BGR'\n",
    "        x = x[..., ::-1]\n",
    "        mean = [103.939, 116.779, 123.68]\n",
    "        std = None\n",
    "\n",
    "        mean_tensor = backend.constant(-np.array(mean))\n",
    "\n",
    "        # Zero-center by mean pixel\n",
    "        if backend.dtype(x) != backend.dtype(mean_tensor):\n",
    "            x = backend.bias_add(\n",
    "                x, backend.cast(mean_tensor, backend.dtype(x)))\n",
    "        else:\n",
    "            x = backend.bias_add(x, mean_tensor)\n",
    "        if std is not None:\n",
    "            x /= std\n",
    "        return x\n",
    "\n",
    "    image = preprocess_image(image)\n",
    "    \n",
    "    \n",
    "    # Return the correct class\n",
    "    image_class_encoded = tf.where(image_class == CAT_CLASS, 1, 0)\n",
    "    return image, image_class_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're rewriting variable names in a lot of places. This is not always\n",
    "# a good practice but we're working in a notebook so: 1) we don't need all variables\n",
    "# but they're still allocated; 2) we can reuse the same name in a different context\n",
    "def initialize_tf_dataset(data, should_batch = True, should_repeat = True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data.image_filename.values, data.image_class.values))\n",
    "    dataset = dataset.map(read_and_prepare_image)\n",
    "    dataset = dataset.shuffle(buffer_size = len(data))\n",
    "    \n",
    "    if should_batch:\n",
    "        dataset = dataset.batch(BATCH_SIZE)\n",
    "    else:\n",
    "        dataset = dataset.batch(len(data))\n",
    "        \n",
    "    if should_repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    return dataset\n",
    "\n",
    "train_data = initialize_tf_dataset(train_data)\n",
    "val_data = initialize_tf_dataset(val_data)\n",
    "test_data = initialize_tf_dataset(test_data, should_batch = False, should_repeat = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at an example to see if it was created correctly\n",
    "for batch in train_data:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a model for transfer learning\n",
    "We'll use ResNet50 as the model base. We could easily omit the \"head\", i.e. the `Dense` layers but in order to demonstrate how to get inputs and outputs from an existing model, we'll do this manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = ResNet50()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the inputs and outputs we want, essentially getting a part of the original model.\n",
    "# This paradigm is widely used, especially when we want to debug or explain a (part of a) model\n",
    "resnet50_conv = Model(inputs = resnet50.get_layer(\"input_1\").input, outputs = resnet50.get_layer(\"avg_pool\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential makes the usage a bit simpler. # Also, adding the resnet\n",
    "# separately allows us to see a shorter summary\n",
    "model = Sequential()\n",
    "model.add(resnet50_conv)\n",
    "model.add(Dense(64, activation = \"relu\"))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(Dense(1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(\"model\").trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're looking at the simplest case, when we have a well-defined loss function\n",
    "# and easy-to-use metrics\n",
    "model.compile(\n",
    "    optimizer = Adam(),\n",
    "    loss = BinaryCrossentropy(),\n",
    "    metrics = [BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "First, we need to train the newly added layers. We might want to add additional callbacks, e.g. for early stopping, changing the learning rate as we train, or stopping training entirely if the loss function (or an output) is `NaN`. It's also pretty common to write our own callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that there might be slightly more, or slightly fewer steps per epoch using this approach.\n",
    "# Since we're running through the entire dataset multiple times, this is not a problem. We could\n",
    "# round up or down, without any significant impact on the training.\n",
    "steps_per_epoch_train = round(len(filenames) * TRAIN_PCT / BATCH_SIZE)\n",
    "steps_per_epoch_val = round(len(filenames) * VAL_PCT / BATCH_SIZE)\n",
    "\n",
    "steps_per_epoch_train, steps_per_epoch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs = 5,\n",
    "    steps_per_epoch = steps_per_epoch_train,\n",
    "    validation_data = val_data,\n",
    "    validation_steps = steps_per_epoch_val,\n",
    "    callbacks = [TensorBoard()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to get to ~95% accuracy on the training set, ~91% on the validation data, and ~84% on the testing set. The model is able to learn relatively easily (even after a few epochs) because it has already been trained on images of animals. Our task is really similar.\n",
    "\n",
    "However, the model tends to overfit the training images too quickly - before being able to learn a lot of useful information (in the `Dense` layers).\n",
    "\n",
    "### Conclusion\n",
    "Usually, we do multiple iteration steps in a single research until we reach a definitive result or prove that it is unattainable (using the given resources).\n",
    "\n",
    "To move forward, we may try a variety of steps, including:\n",
    "* Regularizing the model by decreasing the number of parameters in the classification head\n",
    "* Adding dropout to the classification head to reduce node dependencies\n",
    "* Using a much smaller learning rate (in order to prevent quick overfitting)\n",
    "* Using a different pre-trained model as a starting point\n",
    "* Using one or more of the intermediate outputs of the model: this will allow the classification part to use lower-level features as input. This is not common in the overfitting case but it usually improves generalization performance\n",
    "* Adding more data, by using data augmentation or by additional sampling\n",
    "\n",
    "To debug the model(s), we need to:\n",
    "* Save each (or several of the best) model architectures and weights so we can reuse them\n",
    "* Evaluate and compare their performances\n",
    "* Fine-tune hyperparameters (including model architecture)\n",
    "* Look at right / wrong predictions (in this case, the algorithm didn't seem to struggle with any particular type of image, besides possibly animal posture; it just needed more data to generalize better)\n",
    "* Select one, or even an ensemble of the best models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
